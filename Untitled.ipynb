{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891f6e27-e71d-45fc-9a06-c0328c76cb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 open        high         low       close  \\\n",
      "datetime                                                                    \n",
      "2024-05-03 09:30:00-04:00  186.815002  187.229996  185.429993  186.880798   \n",
      "2024-05-03 10:30:00-04:00  186.895004  187.869995  185.725006  185.773895   \n",
      "2024-05-03 11:30:00-04:00  185.800003  186.789993  185.800003  186.335007   \n",
      "2024-05-03 12:30:00-04:00  186.324997  186.800003  185.950104  186.270004   \n",
      "2024-05-03 13:30:00-04:00  186.270004  186.658203  186.050003  186.414993   \n",
      "\n",
      "                             volume  dividends  stock splits  \n",
      "datetime                                                      \n",
      "2024-05-03 09:30:00-04:00  12693085        0.0           0.0  \n",
      "2024-05-03 10:30:00-04:00   6400873        0.0           0.0  \n",
      "2024-05-03 11:30:00-04:00   3127625        0.0           0.0  \n",
      "2024-05-03 12:30:00-04:00   2636132        0.0           0.0  \n",
      "2024-05-03 13:30:00-04:00   2638140        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Read data and fill missing values\n",
    "data = pd.read_csv(\"candles.csv\", index_col=0)\n",
    "data = data.ffill()\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a6c5c9f-cfff-4d27-b8dd-af288e4e69e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicStockSelector:\n",
    "    def __init__(self, tickers, max_stocks=3):\n",
    "        self.tickers = tickers\n",
    "        self.max_stocks = max_stocks\n",
    "\n",
    "    def fetch_stock_data(self, ticker):\n",
    "        \"\"\"Fetch and process data for a single NYSE stock.\"\"\"\n",
    "        try:\n",
    "            df = yf.Ticker(ticker).history(period=\"1y\", interval=\"1h\")\n",
    "            if df.empty or len(df) < 2:\n",
    "                raise ValueError(\"Insufficient data\")\n",
    "\n",
    "            df = df.reset_index()\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "            volatility = df['close'].pct_change().std()\n",
    "            volume = df['volume'].mean()\n",
    "            liquidity = df['close'].iloc[-1] * df['volume'].iloc[-1]\n",
    "            trend = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]\n",
    "\n",
    "            return {\n",
    "                'ticker': ticker,\n",
    "                'volatility': volatility,\n",
    "                'volume': volume,\n",
    "                'liquidity': liquidity,\n",
    "                'trend': trend\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def apply_filters(self, df):\n",
    "        \"\"\"Apply filters to remove low-performing stocks.\"\"\"\n",
    "        min_volume = df['volume'].quantile(0.25)\n",
    "        df = df[df['volume'] > min_volume]\n",
    "\n",
    "        min_liquidity = df['liquidity'].quantile(0.25)\n",
    "        df = df[df['liquidity'] > min_liquidity]\n",
    "\n",
    "        vol_lower, vol_upper = df['volatility'].quantile([0.25, 0.75])\n",
    "        df = df[(df['volatility'] > vol_lower) & (df['volatility'] < vol_upper)]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def select_stocks(self):\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            stock_data = list(filter(None, executor.map(self.fetch_stock_data, self.tickers)))\n",
    "\n",
    "        df_stocks = pd.DataFrame(stock_data)\n",
    "        if df_stocks.empty:\n",
    "            print(\"No valid stock data collected.\")\n",
    "            return []\n",
    "\n",
    "        df_stocks = self.apply_filters(df_stocks)\n",
    "\n",
    "        # Normalize and score\n",
    "        for column in ['volatility', 'volume', 'liquidity', 'trend']:\n",
    "            df_stocks[column] = (df_stocks[column] - df_stocks[column].min()) / (\n",
    "                df_stocks[column].max() - df_stocks[column].min() + 1e-9)\n",
    "\n",
    "        df_stocks['score'] = (\n",
    "            df_stocks['volatility'] * 0.3 +\n",
    "            df_stocks['volume'] * 0.3 +\n",
    "            df_stocks['liquidity'] * 0.2 +\n",
    "            df_stocks['trend'] * 0.2\n",
    "        )\n",
    "\n",
    "        top_stocks = df_stocks.nlargest(self.max_stocks, 'score')\n",
    "        print(\"Top selected stocks:\")\n",
    "        print(top_stocks[['ticker', 'score']])\n",
    "\n",
    "    \n",
    "        return top_stocks['ticker'].tolist()\n",
    "\n",
    "def select_stocks(tickers=None, max_stocks=3):\n",
    "    \"\"\"\n",
    "    Module-level function to select top-performing stocks.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list, optional): List of stock tickers to analyze. Defaults to popular tech stocks.\n",
    "        max_stocks (int, optional): Maximum number of stocks to return. Defaults to 3.\n",
    "        \n",
    "    Returns:\n",
    "        list: Ticker symbols of top-performing stocks\n",
    "    \"\"\"\n",
    "    if tickers is None:\n",
    "        tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'JNJ', 'JPM', 'V', 'NVDA']\n",
    "    \n",
    "    selector = DynamicStockSelector(tickers=tickers, max_stocks=max_stocks)\n",
    "    return selector.select_stocks()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47981b39-fc52-4a32-819d-7f72767220de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$GOOGL: possibly delisted; no price data found  (period=1y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing GOOGL: Insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AAPL: possibly delisted; no price data found  (period=1y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing AAPL: Insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$V: possibly delisted; no price data found  (period=1y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing V: Insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$AMZN: possibly delisted; no price data found  (period=1y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing AMZN: Insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JNJ: possibly delisted; no price data found  (period=1y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing JNJ: Insufficient data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "$JPM: possibly delisted; no price data found  (period=1y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing JPM: Insufficient data\n",
      "Top selected stocks:\n",
      "Empty DataFrame\n",
      "Columns: [ticker, score]\n",
      "Index: []\n",
      "No tickers returned from top_performer.select_stocks()\n"
     ]
    }
   ],
   "source": [
    "def fetch_candles(ticker, interval, num_candles):\n",
    "    interval_mapping = {\n",
    "        '1m': '1d', '5m': '5d', '15m': '5d', '30m': '5d',\n",
    "        '1h': '1y', '1d': '60d', '1wk': '1y', '1mo': '2y'\n",
    "    }\n",
    "\n",
    "    if interval not in interval_mapping:\n",
    "        raise ValueError(f\"Unsupported interval: {interval}\")\n",
    "\n",
    "    try:\n",
    "        df = yf.Ticker(ticker).history(period=interval_mapping[interval], interval=interval)\n",
    "        df = df.tail(num_candles).reset_index()\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "        # filename = f\"{ticker}_{interval}_{num_candles}_candles.csv\"\n",
    "        filename = f\"candles.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df)} candles for {ticker} to {filename}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- USER SETTINGS ---\n",
    "    interval = \"1h\"         # e.g., \"1d\", \"1h\", \"15m\"\n",
    "    num_candles = 1738        # Number of candles\n",
    "\n",
    "    # --- FETCH TOP STOCKS ---\n",
    "    try:\n",
    "        top_tickers = select_stocks()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling select_stocks(): {e}\")\n",
    "        top_tickers = []\n",
    "\n",
    "    if not top_tickers:\n",
    "        print(\"No tickers returned from top_performer.select_stocks()\")\n",
    "    else:\n",
    "        print(f\"Fetching {num_candles} {interval} candles for:\", \", \".join(top_tickers))\n",
    "        for ticker in top_tickers:\n",
    "            fetch_candles(ticker, interval, num_candles)\n",
    "\n",
    "\n",
    "# data = pd.read_csv('candles.csv',\n",
    "#                    parse_dates=['datetime'],\n",
    "#                    index_col='datetime')\n",
    "# # ensure timezone doesnâ€™t break any numpy ops\n",
    "# data.index = data.index.tz_convert(None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16f16b0c-1c00-453e-9448-28b8bb3046d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                 open        high         low       close  \\\n",
      "datetime                                                                    \n",
      "2024-05-03 09:30:00-04:00  186.815002  187.229996  185.429993  186.880798   \n",
      "2024-05-03 10:30:00-04:00  186.895004  187.869995  185.725006  185.773895   \n",
      "2024-05-03 11:30:00-04:00  185.800003  186.789993  185.800003  186.335007   \n",
      "2024-05-03 12:30:00-04:00  186.324997  186.800003  185.950104  186.270004   \n",
      "2024-05-03 13:30:00-04:00  186.270004  186.658203  186.050003  186.414993   \n",
      "\n",
      "                             volume  dividends  stock splits  \n",
      "datetime                                                      \n",
      "2024-05-03 09:30:00-04:00  12693085        0.0           0.0  \n",
      "2024-05-03 10:30:00-04:00   6400873        0.0           0.0  \n",
      "2024-05-03 11:30:00-04:00   3127625        0.0           0.0  \n",
      "2024-05-03 12:30:00-04:00   2636132        0.0           0.0  \n",
      "2024-05-03 13:30:00-04:00   2638140        0.0           0.0  \n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c3504204-18cd-4f0e-948a-a74d6c49b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "def mse(actual, predicted):\n",
    "    return np.mean((actual-predicted)**2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return (predicted - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ceb1fdd-85d6-49b5-9289-cc0f5671dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahr\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def standard_scale(df, columns):\n",
    "    scaled_df = df[columns].copy()\n",
    "    for column in columns:\n",
    "        mean = scaled_df[column].mean()\n",
    "        std = scaled_df[column].std()\n",
    "        scaled_df[column] = (scaled_df[column] - mean) / std\n",
    "    return scaled_df\n",
    "\n",
    "# Apply scaling\n",
    "# Define predictors and target\n",
    "PREDICTORS = [\"open\", \"high\", \"low\"]\n",
    "TARGET = \"close\"\n",
    "\n",
    "# Scale our data to mean 0\n",
    "data[PREDICTORS] = standard_scale(data, PREDICTORS)\n",
    "\n",
    "# Split into train, valid, test sets\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d46a1192-35b3-49c4-b6b5-cade20221c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        # np.random.seed(0)\n",
    "        k = 1 / math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        \n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3236150-0653-4abc-933d-e7590442de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conf = [\n",
    "    {\"type\": \"input\", \"units\": 3}, # 3 is the number of features\n",
    "    {\"type\": \"rnn\", \"hidden\": 3, \"output\": 1}, # make our 3 features into 4 and give 1 output\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95e26272-bac4-437e-be7a-62b7ecfce27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "def forward(x, layers):\n",
    "    outputs = []\n",
    "    hiddens = []    \n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i] # Get all of the info\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1])) # shape = (num inputs, num of hidden units)\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1])) # shape = (num inputs, num of output units)\n",
    "\n",
    "        for j in range(x.shape[0]): # Go through all of the inputs\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight # apply weights to x\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias # gets current higgen state, apply weights, add biases and current input_x\n",
    "            hidden_x = np.tanh(hidden_x) # activation function\n",
    "            hidden[j,:] = hidden_x\n",
    "\n",
    "            # output \n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            output[j,:] = output_x\n",
    "\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "    return hiddens, outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f73e1c6-f4f2-4d7e-b642-709bb3c3bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]  # Get layer parameters\n",
    "        hidden = hiddens[i]  # Hidden states for current layer\n",
    "        next_h_grad = None\n",
    "\n",
    "        # Initialize gradients\n",
    "        o_weight_grad = np.zeros_like(o_weight)\n",
    "        o_bias_grad = np.zeros_like(o_bias)\n",
    "        h_weight_grad = np.zeros_like(h_weight)\n",
    "        h_bias_grad = np.zeros_like(h_bias)\n",
    "        i_weight_grad = np.zeros_like(i_weight)\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):  # Backprop through time\n",
    "            out_grad = grad[j][np.newaxis, :]  # Shape (1, output_dim)\n",
    "\n",
    "            # Output weight and bias gradient\n",
    "            o_weight_grad += hidden[j][:, np.newaxis] @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate to hidden\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Backprop through next hidden state's gradient\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                h_grad += hh_grad\n",
    "\n",
    "            # Apply tanh derivative\n",
    "            tanh_deriv = 1 - hidden[j][np.newaxis, :] ** 2\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            if j > 0:\n",
    "                h_weight_grad += hidden[j - 1][:, np.newaxis] @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            i_weight_grad += x[j][:, np.newaxis] @ h_grad\n",
    "\n",
    "        # Normalize and apply gradients\n",
    "        scale = lr / x.shape[0]\n",
    "        i_weight -= i_weight_grad * scale\n",
    "        h_weight -= h_weight_grad * scale\n",
    "        h_bias -= h_bias_grad * scale\n",
    "        o_weight -= o_weight_grad * scale\n",
    "        o_bias -= o_bias_grad * scale\n",
    "\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc8f824-56ed-40aa-b4bf-d7b994c57d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss 32980.218794044726 valid loss 30004.828127463476\n",
      "Epoch: 1 train loss 30275.76348110781 valid loss 27869.360479112438\n",
      "Epoch: 2 train loss 27722.274373011933 valid loss 25892.455839760285\n",
      "Epoch: 3 train loss 25382.065408850576 valid loss 24068.048074624417\n",
      "Epoch: 4 train loss 23239.81890440431 valid loss 22384.84817034795\n",
      "Epoch: 5 train loss 21279.421436189365 valid loss 20831.750588530784\n",
      "Epoch: 6 train loss 19485.698822247643 valid loss 19398.372005160792\n",
      "Epoch: 7 train loss 17844.617707358957 valid loss 18075.123766216973\n",
      "Epoch: 8 train loss 16343.270500269413 valid loss 16853.18656892281\n",
      "Epoch: 9 train loss 14969.811975490507 valid loss 15724.459504180708\n",
      "Epoch: 10 train loss 13713.384246810656 valid loss 14681.50403067447\n",
      "Epoch: 11 train loss 12564.040927780507 valid loss 13717.48963852021\n",
      "Epoch: 12 train loss 11512.674257298213 valid loss 12826.143317833463\n",
      "Epoch: 13 train loss 10550.946572986346 valid loss 12001.703248710404\n",
      "Epoch: 14 train loss 9671.226564138547 valid loss 11238.876523871144\n",
      "Epoch: 15 train loss 8866.53032171101 valid loss 10532.800538311309\n",
      "Epoch: 16 train loss 8130.467014975414 valid loss 9879.007667328655\n",
      "Epoch: 17 train loss 7457.18894083413 valid loss 9273.39289570188\n",
      "Epoch: 18 train loss 6841.345659005444 valid loss 8712.184111994822\n",
      "Epoch: 19 train loss 6278.041919175125 valid loss 8191.914826801393\n",
      "Epoch: 20 train loss 5762.79909227298 valid loss 7709.399108444503\n",
      "Epoch: 21 train loss 5291.519830778334 valid loss 7261.708555247019\n",
      "Epoch: 22 train loss 4860.455698891697 valid loss 6846.151142385909\n",
      "Epoch: 23 train loss 4466.177530598491 valid loss 6460.251795796473\n",
      "Epoch: 24 train loss 4105.548291015597 valid loss 6101.734557304244\n",
      "Epoch: 25 train loss 3775.698233344688 valid loss 5768.506215231531\n",
      "Epoch: 26 train loss 3474.0021599314405 valid loss 5458.641283796124\n",
      "Epoch: 27 train loss 3198.0586111791686 valid loss 5170.36822303404\n",
      "Epoch: 28 train loss 2945.6708203223475 valid loss 4902.056798893075\n",
      "Epoch: 29 train loss 2714.8292853149737 valid loss 4652.206490624632\n",
      "Epoch: 30 train loss 2503.695821358323 valid loss 4419.435859665506\n",
      "Epoch: 31 train loss 2310.5889689171217 valid loss 4202.472800849046\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "lr = 1e-5\n",
    "\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sequence_len = 100\n",
    "    epoch_loss = 0\n",
    "    seq_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j: (j + sequence_len),]\n",
    "        seq_y = train_y[j: (j + sequence_len),]\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = mse_grad(seq_y, outputs)\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs)\n",
    "\n",
    "    # if epoch % 50 == 0:\n",
    "    valid_loss = 0\n",
    "    for j in range(valid_x.shape[0] - sequence_len):\n",
    "        seq_x = valid_x[j: (j+sequence_len),]\n",
    "        seq_y = valid_y[j: (j+sequence_len),]\n",
    "        _, outputs = forward(seq_x, layers)\n",
    "        valid_loss += mse(seq_y,outputs)\n",
    "    print(f\"Epoch: {epoch} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6e833-3ed5-48ef-b7df-324a16a0d5b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_len = 100\n",
    "j = valid_x.shape[0] - sequence_len\n",
    "seq_x = valid_x[j: (j + sequence_len),]\n",
    "seq_y = valid_y[j: (j + sequence_len),]\n",
    "_, outputs = forward(seq_x, layers)\n",
    "\n",
    "# Convert to 1D if needed\n",
    "actual = seq_y.squeeze()\n",
    "predicted = outputs.squeeze()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(actual, label=\"Actual\")\n",
    "plt.plot(predicted, label=\"Predicted\")\n",
    "plt.title(\"Last Validation Sequence: Predicted vs Actual\")\n",
    "plt.xlabel(\"Time Step\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 (Good)",
   "language": "python",
   "name": "py3129"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

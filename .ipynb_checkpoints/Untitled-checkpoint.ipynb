{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "891f6e27-e71d-45fc-9a06-c0328c76cb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# !pip install cupy-cuda12x\n",
    "\n",
    "# import cupy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Read data and fill missing values\n",
    "data = pd.read_csv(\"clean_weather.csv\", index_col=0)\n",
    "data = data.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2a6c5c9f-cfff-4d27-b8dd-af288e4e69e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yfinance'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myfinance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01myf\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'yfinance'"
     ]
    }
   ],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class DynamicStockSelector:\n",
    "    def __init__(self, tickers, max_stocks=3):\n",
    "        self.tickers = tickers\n",
    "        self.max_stocks = max_stocks\n",
    "\n",
    "    def fetch_stock_data(self, ticker):\n",
    "        \"\"\"Fetch and process data for a single NYSE stock.\"\"\"\n",
    "        try:\n",
    "            df = yf.Ticker(ticker).history(period=\"2d\", interval=\"1h\")\n",
    "            if df.empty or len(df) < 2:\n",
    "                raise ValueError(\"Insufficient data\")\n",
    "\n",
    "            df = df.reset_index()\n",
    "            df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "            volatility = df['close'].pct_change().std()\n",
    "            volume = df['volume'].mean()\n",
    "            liquidity = df['close'].iloc[-1] * df['volume'].iloc[-1]\n",
    "            trend = (df['close'].iloc[-1] - df['close'].iloc[0]) / df['close'].iloc[0]\n",
    "\n",
    "            return {\n",
    "                'ticker': ticker,\n",
    "                'volatility': volatility,\n",
    "                'volume': volume,\n",
    "                'liquidity': liquidity,\n",
    "                'trend': trend\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {ticker}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def apply_filters(self, df):\n",
    "        \"\"\"Apply filters to remove low-performing stocks.\"\"\"\n",
    "        min_volume = df['volume'].quantile(0.25)\n",
    "        df = df[df['volume'] > min_volume]\n",
    "\n",
    "        min_liquidity = df['liquidity'].quantile(0.25)\n",
    "        df = df[df['liquidity'] > min_liquidity]\n",
    "\n",
    "        vol_lower, vol_upper = df['volatility'].quantile([0.25, 0.75])\n",
    "        df = df[(df['volatility'] > vol_lower) & (df['volatility'] < vol_upper)]\n",
    "\n",
    "        return df\n",
    "\n",
    "    def select_stocks(self):\n",
    "        with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            stock_data = list(filter(None, executor.map(self.fetch_stock_data, self.tickers)))\n",
    "\n",
    "        df_stocks = pd.DataFrame(stock_data)\n",
    "        if df_stocks.empty:\n",
    "            print(\"No valid stock data collected.\")\n",
    "            return []\n",
    "\n",
    "        df_stocks = self.apply_filters(df_stocks)\n",
    "\n",
    "        # Normalize and score\n",
    "        for column in ['volatility', 'volume', 'liquidity', 'trend']:\n",
    "            df_stocks[column] = (df_stocks[column] - df_stocks[column].min()) / (\n",
    "                df_stocks[column].max() - df_stocks[column].min() + 1e-9)\n",
    "\n",
    "        df_stocks['score'] = (\n",
    "            df_stocks['volatility'] * 0.3 +\n",
    "            df_stocks['volume'] * 0.3 +\n",
    "            df_stocks['liquidity'] * 0.2 +\n",
    "            df_stocks['trend'] * 0.2\n",
    "        )\n",
    "\n",
    "        top_stocks = df_stocks.nlargest(self.max_stocks, 'score')\n",
    "        print(\"Top selected stocks:\")\n",
    "        print(top_stocks[['ticker', 'score']])\n",
    "\n",
    "    \n",
    "        return top_stocks['ticker'].tolist()\n",
    "\n",
    "def select_stocks(tickers=None, max_stocks=3):\n",
    "    \"\"\"\n",
    "    Module-level function to select top-performing stocks.\n",
    "    \n",
    "    Args:\n",
    "        tickers (list, optional): List of stock tickers to analyze. Defaults to popular tech stocks.\n",
    "        max_stocks (int, optional): Maximum number of stocks to return. Defaults to 3.\n",
    "        \n",
    "    Returns:\n",
    "        list: Ticker symbols of top-performing stocks\n",
    "    \"\"\"\n",
    "    if tickers is None:\n",
    "        tickers = ['AAPL', 'MSFT', 'GOOGL', 'AMZN', 'META', 'TSLA', 'JNJ', 'JPM', 'V', 'NVDA']\n",
    "    \n",
    "    selector = DynamicStockSelector(tickers=tickers, max_stocks=max_stocks)\n",
    "    return selector.select_stocks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47981b39-fc52-4a32-819d-7f72767220de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import top_performer  \n",
    "\n",
    "def fetch_candles(ticker, interval, num_candles):\n",
    "    interval_mapping = {\n",
    "        '1m': '1d', '5m': '5d', '15m': '5d', '30m': '5d',\n",
    "        '1h': '7d', '1d': '60d', '1wk': '1y', '1mo': '2y'\n",
    "    }\n",
    "\n",
    "    if interval not in interval_mapping:\n",
    "        raise ValueError(f\"Unsupported interval: {interval}\")\n",
    "\n",
    "    try:\n",
    "        df = yf.Ticker(ticker).history(period=interval_mapping[interval], interval=interval)\n",
    "        df = df.tail(num_candles).reset_index()\n",
    "        df.columns = [col.lower() for col in df.columns]\n",
    "\n",
    "        filename = f\"{ticker}_{interval}_{num_candles}_candles.csv\"\n",
    "        df.to_csv(filename, index=False)\n",
    "        print(f\"Saved {len(df)} candles for {ticker} to {filename}\")\n",
    "        return df\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to fetch data for {ticker}: {e}\")\n",
    "        return None\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # --- USER SETTINGS ---\n",
    "    interval = \"1h\"         # e.g., \"1d\", \"1h\", \"15m\"\n",
    "    num_candles = 48        # Number of candles\n",
    "\n",
    "    # --- FETCH TOP STOCKS ---\n",
    "    try:\n",
    "        top_tickers = top_performer.select_stocks()\n",
    "    except Exception as e:\n",
    "        print(f\"Error calling select_stocks(): {e}\")\n",
    "        top_tickers = []\n",
    "\n",
    "    if not top_tickers:\n",
    "        print(\"No tickers returned from top_performer.select_stocks()\")\n",
    "    else:\n",
    "        print(f\"Fetching {num_candles} {interval} candles for:\", \", \".join(top_tickers))\n",
    "        for ticker in top_tickers:\n",
    "            fetch_candles(ticker, interval, num_candles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16f16b0c-1c00-453e-9448-28b8bb3046d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            tmax  tmin  rain  tmax_tomorrow\n",
      "1970-01-01  60.0  35.0   0.0           52.0\n",
      "1970-01-02  52.0  39.0   0.0           52.0\n",
      "1970-01-03  52.0  35.0   0.0           53.0\n",
      "1970-01-04  53.0  36.0   0.0           52.0\n",
      "1970-01-05  52.0  35.0   0.0           50.0\n"
     ]
    }
   ],
   "source": [
    "#Data\n",
    "\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f19a804-9bb8-4b04-85c6-bb6f3b178605",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([66., 70., 62.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "i_weight = np.random.rand(1,2)\n",
    "h_weight = np.random.rand(2,2)\n",
    "o_weight = np.random.rand(2,1)\n",
    "\n",
    "temps = data[\"tmax\"].tail(3).to_numpy()\n",
    "temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d08ad9b7-7dfc-49d8-9864-5bb08417ed3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[66.]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "\n",
    "x0 = temps[0].reshape(1,1)\n",
    "x1 = temps[1].reshape(1,1)\n",
    "x2 = temps[2].reshape(1,1)\n",
    "\n",
    "x0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "05158b18-5a74-4f47-aa9f-dc7e15d526f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.22169126, 47.20249818]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "\n",
    "#LAYER 1\n",
    "xi_0 = x0 @ i_weight\n",
    "\n",
    "xi_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c692152-cfa5-4eb9-9b04-a963053ea9f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[36.22169126, 47.20249818]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "\n",
    "xh_0 = np.maximum(0,xi_0) #relu\n",
    "\n",
    "xh_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef58adc9-92e4-4d98-b43e-8ee935893c5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[57.94406231]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "\n",
    "xo_0 = xh_0 @ o_weight\n",
    "xo_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67d20eff-4049-4db1-ac0a-cc6e223ea7d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[124.54916092]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "\n",
    "#Layer 1\n",
    "xi_1 = x1 @ i_weight # apply weights on input x1\n",
    "\n",
    "xh = xh_0 @ h_weight # Update the hidden Layer\n",
    "xh_1 = np.maximum(0, xh + xi_1) # RELU for the hidden layer\n",
    "\n",
    "xo_1 = xh_1 @ o_weight\n",
    "\n",
    "xo_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "211fea28-abd4-4945-a45a-be307266b4dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[190.94853131]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Manual Forward Pass (Dont need this anymore)\n",
    "\n",
    "\n",
    "#Layer 2\n",
    "xi_2 = x2 @ i_weight # apply weights on input x1\n",
    "\n",
    "xh = xh_1 @ h_weight # Update the hidden Layer\n",
    "xh_2 = np.maximum(0, xh + xi_2) # RELU for the hidden layer\n",
    "\n",
    "xo_2 = xh_2 @ o_weight\n",
    "\n",
    "xo_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3ef1945c-182d-491c-a1fd-10d7a80fcee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize variables\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "i_weight = np.random.rand(1,5) / 5 - .1\n",
    "h_weight = np.random.rand(5,5) / 5 - .1\n",
    "h_bias = np.random.rand(1,5) / 5 - .1\n",
    "o_weight = np.random.rand(5,1) * 50\n",
    "o_bias = np.random.rand(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be588d6e-ef7d-40b6-9251-80c7f8147060",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "\n",
    "outputs = np.zeros(3)\n",
    "hiddens = np.zeros((3,5))\n",
    "prev_hidden = None\n",
    "sequence = data[\"tmax\"].tail(3).to_numpy()\n",
    "\n",
    "for i in range(3):\n",
    "    x = sequence[i].reshape(1,1)\n",
    "    xi = x @ i_weight\n",
    "\n",
    "    if prev_hidden is None: # check if it is the first iteration\n",
    "        xh = xi\n",
    "    else:\n",
    "        xh = xi + prev_hidden @ h_weight + h_bias\n",
    "    \n",
    "    xh = np.tanh(xh) # apply activation function to hidden layer\n",
    "    prev_hidden = xh\n",
    "    hiddens[i,] = xh # save the hidden layer for back propogation\n",
    "    xo = xh @ o_weight + o_bias\n",
    "    outputs[i] = xo.item()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc4b8989-d456-4cb9-a367-7d26010f0d8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([74.31470595, 80.66149404, 77.67852446])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c3504204-18cd-4f0e-948a-a74d6c49b4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "def mse(actual, predicted):\n",
    "    return np.mean((actual-predicted)**2)\n",
    "\n",
    "def mse_grad(actual, predicted):\n",
    "    return (predicted - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d8ba2b96-a300-4afa-85b3-8d546d3d3493",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.31470595, 18.66149404, 12.67852446])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actuals = np.array([70,62,65])\n",
    "\n",
    "loss_grad = mse_grad(actuals, outputs)\n",
    "loss_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "63bd39ea-157d-453e-bb82-154ec87eda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "next_hidden = None\n",
    "\n",
    "o_weight_grad, o_bias_grad, h_weight_grad, h_bias_grad, i_weight_grad =  [0] * 5\n",
    "\n",
    "for i in range(2, -1, -1):\n",
    "    l_grad = loss_grad[i].reshape(1,1)\n",
    "\n",
    "    o_weight_grad += hiddens[i][:,np.newaxis] @ l_grad\n",
    "    o_bias_grad += np.mean(l_grad)\n",
    "\n",
    "    o_grad = l_grad @ o_weight.T\n",
    "    if next_hidden is None:\n",
    "        h_grad = o_grad\n",
    "    else:\n",
    "        h_grad = o_grad + next_hidden @ h_weight.T\n",
    "\n",
    "    tanh_deriv = 1 - hiddens[i,:][np.newaxis,:]\n",
    "    h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "    next_hidden = h_grad\n",
    "\n",
    "    if i > 0:\n",
    "        h_weight_grad += hiddens[i-1,:][:,np.newaxis] @ h_grad\n",
    "        h_bias_grad += np.mean(h_grad)\n",
    "\n",
    "    i_weight_grad += sequence[i].reshape(1,1).T @ h_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f863db7-9ad1-43b3-a31b-ff9ee71846cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 32352.67091083,    392.99655233,   9528.43418739,\n",
       "         35514.29142052, 146401.10347147]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i_weight_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7ceb1fdd-85d6-49b5-9289-cc0f5671dce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shahr\\AppData\\Roaming\\Python\\Python312\\site-packages\\numpy\\_core\\fromnumeric.py:57: FutureWarning: 'DataFrame.swapaxes' is deprecated and will be removed in a future version. Please use 'DataFrame.transpose' instead.\n",
      "  return bound(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def standard_scale(df, columns):\n",
    "    scaled_df = df[columns].copy()\n",
    "    for column in columns:\n",
    "        mean = scaled_df[column].mean()\n",
    "        std = scaled_df[column].std()\n",
    "        scaled_df[column] = (scaled_df[column] - mean) / std\n",
    "    return scaled_df\n",
    "\n",
    "# Apply scaling\n",
    "# Define predictors and target\n",
    "PREDICTORS = [\"tmax\", \"tmin\", \"rain\"]\n",
    "TARGET = \"tmax_tomorrow\"\n",
    "\n",
    "# Scale our data to mean 0\n",
    "data[PREDICTORS] = standard_scale(data, PREDICTORS)\n",
    "\n",
    "# Split into train, valid, test sets\n",
    "np.random.seed(0)\n",
    "split_data = np.split(data, [int(.7*len(data)), int(.85*len(data))])\n",
    "(train_x, train_y), (valid_x, valid_y), (test_x, test_y) = [[d[PREDICTORS].to_numpy(), d[[TARGET]].to_numpy()] for d in split_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d46a1192-35b3-49c4-b6b5-cade20221c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(layer_conf):\n",
    "    layers = []\n",
    "    for i in range(1, len(layer_conf)):\n",
    "        np.random.seed(0)\n",
    "        k = 1 / math.sqrt(layer_conf[i][\"hidden\"])\n",
    "        \n",
    "        i_weight = np.random.rand(layer_conf[i-1][\"units\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "        h_bias = np.random.rand(1, layer_conf[i][\"hidden\"]) * 2 * k - k\n",
    "\n",
    "        o_weight = np.random.rand(layer_conf[i][\"hidden\"], layer_conf[i][\"output\"]) * 2 * k - k\n",
    "        o_bias = np.random.rand(1, layer_conf[i][\"output\"]) * 2 * k - k\n",
    "\n",
    "        layers.append(\n",
    "            [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "        )\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3236150-0653-4abc-933d-e7590442de0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_conf = [\n",
    "    {\"type\": \"input\", \"units\": 3}, # 3 is the number of features\n",
    "    {\"type\": \"rnn\", \"hidden\": 3, \"output\": 1}, # make our 3 features into 4 and give 1 output\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "95e26272-bac4-437e-be7a-62b7ecfce27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward Pass\n",
    "def forward(x, layers):\n",
    "    outputs = []\n",
    "    hiddens = []    \n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i] # Get all of the info\n",
    "        hidden = np.zeros((x.shape[0], i_weight.shape[1])) # shape = (num inputs, num of hidden units)\n",
    "        output = np.zeros((x.shape[0], o_weight.shape[1])) # shape = (num inputs, num of output units)\n",
    "\n",
    "        for j in range(x.shape[0]): # Go through all of the inputs\n",
    "            input_x = x[j,:][np.newaxis,:] @ i_weight # apply weights to x\n",
    "            hidden_x = input_x + hidden[max(j-1,0),:][np.newaxis,:] @ h_weight + h_bias # gets current higgen state, apply weights, add biases and current input_x\n",
    "            hidden_x = np.tanh(hidden_x) # activation function\n",
    "            hidden[j,:] = hidden_x\n",
    "\n",
    "            # output \n",
    "            output_x = hidden_x @ o_weight + o_bias\n",
    "            output[j,:] = output_x\n",
    "\n",
    "        hiddens.append(hidden)\n",
    "        outputs.append(output)\n",
    "    return hiddens, outputs[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8f73e1c6-f4f2-4d7e-b642-709bb3c3bbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Backward Pass\n",
    "def backward(layers, x, lr, grad, hiddens):\n",
    "    for i in range(len(layers)):\n",
    "        i_weight, h_weight, h_bias, o_weight, o_bias = layers[i]  # Get layer parameters\n",
    "        hidden = hiddens[i]  # Hidden states for current layer\n",
    "        next_h_grad = None\n",
    "\n",
    "        # Initialize gradients\n",
    "        o_weight_grad = np.zeros_like(o_weight)\n",
    "        o_bias_grad = np.zeros_like(o_bias)\n",
    "        h_weight_grad = np.zeros_like(h_weight)\n",
    "        h_bias_grad = np.zeros_like(h_bias)\n",
    "        i_weight_grad = np.zeros_like(i_weight)\n",
    "\n",
    "        for j in range(x.shape[0] - 1, -1, -1):  # Backprop through time\n",
    "            out_grad = grad[j][np.newaxis, :]  # Shape (1, output_dim)\n",
    "\n",
    "            # Output weight and bias gradient\n",
    "            o_weight_grad += hidden[j][:, np.newaxis] @ out_grad\n",
    "            o_bias_grad += out_grad\n",
    "\n",
    "            # Propagate to hidden\n",
    "            h_grad = out_grad @ o_weight.T\n",
    "\n",
    "            if j < x.shape[0] - 1:\n",
    "                # Backprop through next hidden state's gradient\n",
    "                hh_grad = next_h_grad @ h_weight.T\n",
    "                h_grad += hh_grad\n",
    "\n",
    "            # Apply tanh derivative\n",
    "            tanh_deriv = 1 - hidden[j][np.newaxis, :] ** 2\n",
    "            h_grad = np.multiply(h_grad, tanh_deriv)\n",
    "\n",
    "            next_h_grad = h_grad.copy()\n",
    "\n",
    "            if j > 0:\n",
    "                h_weight_grad += hidden[j - 1][:, np.newaxis] @ h_grad\n",
    "                h_bias_grad += h_grad\n",
    "\n",
    "            i_weight_grad += x[j][:, np.newaxis] @ h_grad\n",
    "\n",
    "        # Normalize and apply gradients\n",
    "        scale = lr / x.shape[0]\n",
    "        i_weight -= i_weight_grad * scale\n",
    "        h_weight -= h_weight_grad * scale\n",
    "        h_bias -= h_bias_grad * scale\n",
    "        o_weight -= o_weight_grad * scale\n",
    "        o_bias -= o_bias_grad * scale\n",
    "\n",
    "        layers[i] = [i_weight, h_weight, h_bias, o_weight, o_bias]\n",
    "\n",
    "    return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0fc8f824-56ed-40aa-b4bf-d7b994c57d65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 train loss 3433.71089821061 valid loss 2644.251124146831\n",
      "Epoch: 1 train loss 1771.443825637194 valid loss 1358.8473333515356\n",
      "Epoch: 2 train loss 894.5481972025864 valid loss 719.3313070693455\n",
      "Epoch: 3 train loss 468.8375400812705 valid loss 401.17169571582457\n",
      "Epoch: 4 train loss 263.29661326443454 valid loss 241.643701565443\n",
      "Epoch: 5 train loss 164.29171328901373 valid loss 160.6026010843097\n",
      "Epoch: 6 train loss 116.67961741772037 valid loss 118.71097744097042\n",
      "Epoch: 7 train loss 93.84596915275891 valid loss 96.6091630463487\n",
      "Epoch: 8 train loss 82.98127188970948 valid loss 84.70792008722387\n",
      "Epoch: 9 train loss 77.93358105537212 valid loss 78.21579757013822\n",
      "Epoch: 10 train loss 75.76313935757865 valid loss 74.72324814499964\n",
      "Epoch: 11 train loss 75.09601168951343 valid loss 73.04256649500043\n",
      "Epoch: 12 train loss 75.37166116231857 valid loss 72.71765087187686\n",
      "Epoch: 13 train loss 76.65796969496003 valid loss 74.28906552108091\n",
      "Epoch: 14 train loss 75.90095331411781 valid loss 74.67444519704216\n",
      "Epoch: 15 train loss 66.80191487022236 valid loss 66.44572442409995\n",
      "Epoch: 16 train loss 59.15210589000993 valid loss 58.92980724880917\n",
      "Epoch: 17 train loss 53.75849993325445 valid loss 53.58900412720387\n",
      "Epoch: 18 train loss 49.81821144473681 valid loss 49.58006079591688\n",
      "Epoch: 19 train loss 46.75226240859671 valid loss 46.422750308032505\n",
      "Epoch: 20 train loss 44.25556072127808 valid loss 43.84223290248531\n",
      "Epoch: 21 train loss 42.156414297615285 valid loss 41.67418564295418\n",
      "Epoch: 22 train loss 40.35191888393422 valid loss 39.815645484878274\n",
      "Epoch: 23 train loss 38.77652106262255 valid loss 38.19879499602462\n",
      "Epoch: 24 train loss 37.385941845432505 valid loss 36.776703869692604\n",
      "Epoch: 25 train loss 36.14864095420055 valid loss 35.51540632974864\n",
      "Epoch: 26 train loss 35.04111407033544 valid loss 34.38936748422256\n",
      "Epoch: 27 train loss 34.045191919074 valid loss 33.378788430971134\n",
      "Epoch: 28 train loss 33.14641582647872 valid loss 32.46793345261864\n",
      "Epoch: 29 train loss 32.33301302787877 valid loss 31.644044957963175\n",
      "Epoch: 30 train loss 31.595220284832266 valid loss 30.896610363489764\n",
      "Epoch: 31 train loss 30.924819011650776 valid loss 30.21684888381127\n",
      "Epoch: 32 train loss 30.314804602624744 valid loss 29.59734143528327\n",
      "Epoch: 33 train loss 29.75914439253981 valid loss 29.031757107424266\n",
      "Epoch: 34 train loss 29.252596229154413 valid loss 28.51464683231425\n",
      "Epoch: 35 train loss 28.79056973276047 valid loss 28.041285031193393\n",
      "Epoch: 36 train loss 28.36901837245584 valid loss 27.607546259211297\n",
      "Epoch: 37 train loss 27.984354264346774 valid loss 27.2098078537249\n",
      "Epoch: 38 train loss 27.63338003493067 valid loss 26.844872220150233\n",
      "Epoch: 39 train loss 27.31323371504547 valid loss 26.50990417300852\n",
      "Epoch: 40 train loss 27.02134373658107 valid loss 26.202379989129188\n",
      "Epoch: 41 train loss 26.755391875690105 valid loss 25.920045708461455\n",
      "Epoch: 42 train loss 26.51328253425779 valid loss 25.660882851672113\n",
      "Epoch: 43 train loss 26.293117147576886 valid loss 25.42308018868776\n",
      "Epoch: 44 train loss 26.093172797748263 valid loss 25.205010540438398\n",
      "Epoch: 45 train loss 25.911884331266613 valid loss 25.00521186448392\n",
      "Epoch: 46 train loss 25.74782944772451 valid loss 24.82237209256569\n",
      "Epoch: 47 train loss 25.59971636027625 valid loss 24.65531737858363\n",
      "Epoch: 48 train loss 25.466373738884833 valid loss 24.503003602234394\n",
      "Epoch: 49 train loss 25.346742742640494 valid loss 24.364511182477244\n",
      "Epoch: 50 train loss 25.23987103288978 valid loss 24.239043520050632\n",
      "Epoch: 51 train loss 25.144908735566766 valid loss 24.125929760250834\n",
      "Epoch: 52 train loss 25.06110638095976 valid loss 24.024633129545716\n",
      "Epoch: 53 train loss 24.98781486196512 valid loss 23.934766998263324\n",
      "Epoch: 54 train loss 24.924487329519526 valid loss 23.856122322032277\n",
      "Epoch: 55 train loss 24.870682442119524 valid loss 23.788712713297848\n",
      "Epoch: 56 train loss 24.826066823797085 valid loss 23.73284803024589\n",
      "Epoch: 57 train loss 24.79040999767392 valid loss 23.689255783405446\n",
      "Epoch: 58 train loss 24.76355150025327 valid loss 23.659284675867898\n",
      "Epoch: 59 train loss 24.745278197867382 valid loss 23.645247950721327\n",
      "Epoch: 60 train loss 24.734914672117643 valid loss 23.650974195787548\n",
      "Epoch: 61 train loss 24.72997480726439 valid loss 23.682415583711197\n",
      "Epoch: 62 train loss 24.721848923984485 valid loss 23.74615213763107\n",
      "Epoch: 63 train loss 24.68606548510957 valid loss 23.83287292569296\n",
      "Epoch: 64 train loss 24.601528366892705 valid loss 23.885387828281875\n",
      "Epoch: 65 train loss 24.5150805971306 valid loss 23.872210587891303\n",
      "Epoch: 66 train loss 24.418387088108066 valid loss 23.801783599881862\n",
      "Epoch: 67 train loss 24.299422172766935 valid loss 23.688286738308587\n",
      "Epoch: 68 train loss 24.161295428945095 valid loss 23.546161825737165\n",
      "Epoch: 69 train loss 24.010525011236872 valid loss 23.386971113326116\n",
      "Epoch: 70 train loss 23.853323493246016 valid loss 23.219101386626065\n",
      "Epoch: 71 train loss 23.694576815573285 valid loss 23.04842300528331\n",
      "Epoch: 72 train loss 23.53783809806035 valid loss 22.87898161304595\n",
      "Epoch: 73 train loss 23.385572855738367 valid loss 22.71351945659877\n",
      "Epoch: 74 train loss 23.23941299777634 valid loss 22.553844315286042\n",
      "Epoch: 75 train loss 23.100368034197768 valid loss 22.401092221666282\n",
      "Epoch: 76 train loss 22.968991233733767 valid loss 22.255917661327913\n",
      "Epoch: 77 train loss 22.845507543510347 valid loss 22.118632583714618\n",
      "Epoch: 78 train loss 22.729910580986136 valid loss 21.989308180800705\n",
      "Epoch: 79 train loss 22.62203515702473 valid loss 21.867849199696174\n",
      "Epoch: 80 train loss 22.52161079814242 valid loss 21.754047942928146\n",
      "Epoch: 81 train loss 22.428300762470396 valid loss 21.647623291221887\n",
      "Epoch: 82 train loss 22.341730139011126 valid loss 21.548248732365373\n",
      "Epoch: 83 train loss 22.261505825716696 valid loss 21.455572355380056\n",
      "Epoch: 84 train loss 22.187230520665842 valid loss 21.369230994338892\n",
      "Epoch: 85 train loss 22.1185123321401 valid loss 21.288860125492175\n",
      "Epoch: 86 train loss 22.05497120344501 valid loss 21.214100690260015\n",
      "Epoch: 87 train loss 21.996243036800834 valid loss 21.144603698975445\n",
      "Epoch: 88 train loss 21.941982167064303 valid loss 21.080033237296398\n",
      "Epoch: 89 train loss 21.891862662400595 valid loss 21.020068326915915\n",
      "Epoch: 90 train loss 21.845578800578167 valid loss 20.96440396801586\n",
      "Epoch: 91 train loss 21.802844974839964 valid loss 20.912751600510052\n",
      "Epoch: 92 train loss 21.763395213605293 valid loss 20.864839155480592\n",
      "Epoch: 93 train loss 21.726982447080324 valid loss 20.82041082067101\n",
      "Epoch: 94 train loss 21.69337761636173 valid loss 20.779226609561395\n",
      "Epoch: 95 train loss 21.662368693223712 valid loss 20.741061798816077\n",
      "Epoch: 96 train loss 21.633759658816786 valid loss 20.70570628109627\n",
      "Epoch: 97 train loss 21.60736947500793 valid loss 20.67296386741255\n",
      "Epoch: 98 train loss 21.58303107156829 valid loss 20.64265156392905\n",
      "Epoch: 99 train loss 21.56059036481267 valid loss 20.614598841384804\n",
      "Epoch: 100 train loss 21.53990531779321 valid loss 20.58864691033809\n",
      "Epoch: 101 train loss 21.520845048182977 valid loss 20.56464801174669\n",
      "Epoch: 102 train loss 21.50328898713194 valid loss 20.54246472960883\n",
      "Epoch: 103 train loss 21.48712609033228 valid loss 20.521969330268437\n",
      "Epoch: 104 train loss 21.47225410108474 valid loss 20.503043131355394\n",
      "Epoch: 105 train loss 21.458578864137483 valid loss 20.485575902074896\n",
      "Epoch: 106 train loss 21.446013688390526 valid loss 20.469465295600155\n",
      "Epoch: 107 train loss 21.434478756108568 valid loss 20.454616313590815\n",
      "Epoch: 108 train loss 21.42390057602448 valid loss 20.44094080232058\n",
      "Epoch: 109 train loss 21.41421147758993 valid loss 20.428356979505768\n",
      "Epoch: 110 train loss 21.405349143598674 valid loss 20.41678899065796\n",
      "Epoch: 111 train loss 21.397256178460857 valid loss 20.40616649361655\n",
      "Epoch: 112 train loss 21.38987970950158 valid loss 20.396424269818777\n",
      "Epoch: 113 train loss 21.383171018794613 valid loss 20.387501860830557\n",
      "Epoch: 114 train loss 21.37708520319874 valid loss 20.379343228671804\n",
      "Epoch: 115 train loss 21.37158086043519 valid loss 20.371896438506482\n",
      "Epoch: 116 train loss 21.36661979922017 valid loss 20.365113362330977\n",
      "Epoch: 117 train loss 21.362166771637074 valid loss 20.35894940236916\n",
      "Epoch: 118 train loss 21.3581892261082 valid loss 20.3533632329626\n",
      "Epoch: 119 train loss 21.354657079472428 valid loss 20.348316559829073\n",
      "Epoch: 120 train loss 21.35154250683478 valid loss 20.343773895643096\n",
      "Epoch: 121 train loss 21.348819747977686 valid loss 20.339702350968288\n",
      "Epoch: 122 train loss 21.346464929251695 valid loss 20.336071439644137\n",
      "Epoch: 123 train loss 21.344455899957833 valid loss 20.332852897788158\n",
      "Epoch: 124 train loss 21.342772082336616 valid loss 20.330020515630853\n",
      "Epoch: 125 train loss 21.341394334338695 valid loss 20.327549981443948\n",
      "Epoch: 126 train loss 21.3403048244218 valid loss 20.32541873685747\n",
      "Epoch: 127 train loss 21.339486917653307 valid loss 20.32360584288668\n",
      "Epoch: 128 train loss 21.33892507242756 valid loss 20.322091856005503\n",
      "Epoch: 129 train loss 21.338604747113177 valid loss 20.32085871361246\n",
      "Epoch: 130 train loss 21.338512315941042 valid loss 20.31988962822947\n",
      "Epoch: 131 train loss 21.338634993415205 valid loss 20.31916898977103\n",
      "Epoch: 132 train loss 21.33896076649251 valid loss 20.318682275202438\n",
      "Epoch: 133 train loss 21.33947833372209 valid loss 20.318415964890477\n",
      "Epoch: 134 train loss 21.340177050469755 valid loss 20.318357464932\n",
      "Epoch: 135 train loss 21.341046879291603 valid loss 20.318495034734656\n",
      "Epoch: 136 train loss 21.34207834445206 valid loss 20.318817719122887\n",
      "Epoch: 137 train loss 21.343262489541537 valid loss 20.31931528426172\n",
      "Epoch: 138 train loss 21.34459083713088 valid loss 20.31997815673796\n",
      "Epoch: 139 train loss 21.34605534943134 valid loss 20.320797365219338\n",
      "Epoch: 140 train loss 21.34764838903022 valid loss 20.32176448424008\n",
      "Epoch: 141 train loss 21.349362678934458 valid loss 20.322871579831588\n",
      "Epoch: 142 train loss 21.351191261428653 valid loss 20.324111156932453\n",
      "Epoch: 143 train loss 21.35312745559629 valid loss 20.325476108765965\n",
      "Epoch: 144 train loss 21.355164813777034 valid loss 20.326959668636672\n",
      "Epoch: 145 train loss 21.35729707771157 valid loss 20.328555364857632\n",
      "Epoch: 146 train loss 21.35951813559607 valid loss 20.330256979728414\n",
      "Epoch: 147 train loss 21.361821981698707 valid loss 20.332058513615717\n",
      "Epoch: 148 train loss 21.364202680491132 valid loss 20.333954155195944\n",
      "Epoch: 149 train loss 21.366654337380858 valid loss 20.33593825879346\n",
      "Epoch: 150 train loss 21.369171078026906 valid loss 20.338005329467567\n",
      "Epoch: 151 train loss 21.37174703787641 valid loss 20.34015001609232\n",
      "Epoch: 152 train loss 21.374376362975035 valid loss 20.342367112169708\n",
      "Epoch: 153 train loss 21.377053222341623 valid loss 20.344651563573596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 150\n",
    "lr = 1e-5\n",
    "\n",
    "layers = init_params(layer_conf)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    sequence_len = 7\n",
    "    epoch_loss = 0\n",
    "    seq_loss = 0\n",
    "    for j in range(train_x.shape[0] - sequence_len):\n",
    "        seq_x = train_x[j: (j + sequence_len),]\n",
    "        seq_y = train_y[j: (j + sequence_len),]\n",
    "\n",
    "        hiddens, outputs = forward(seq_x, layers)\n",
    "        grad = mse_grad(seq_y, outputs)\n",
    "        params = backward(layers, seq_x, lr, grad, hiddens)\n",
    "        epoch_loss += mse(seq_y, outputs)\n",
    "\n",
    "    # if epoch % 50 == 0:\n",
    "    valid_loss = 0\n",
    "    for j in range(valid_x.shape[0] - sequence_len):\n",
    "        seq_x = valid_x[j: (j+sequence_len),]\n",
    "        seq_y = valid_y[j: (j+sequence_len),]\n",
    "        _, outputs = forward(seq_x, layers)\n",
    "        valid_loss += mse(seq_y,outputs)\n",
    "    print(f\"Epoch: {epoch} train loss {epoch_loss / len(train_x)} valid loss {valid_loss / len(valid_x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd6e833-3ed5-48ef-b7df-324a16a0d5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.12.9 (Good)",
   "language": "python",
   "name": "py3129"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
